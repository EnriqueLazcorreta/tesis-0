% !TEX root = ../../../Lazcorreta.Tesis.tex
% \ABIERTO%

% TODO: Añadir \descripcion al index
Llegados a esta fase hemos de decidir qué tarea de \dm se adapta mejor a los datos que tenemos y al objetivo final del estudio: \clustering (\cite{NgHan-EfficientAndEffectiveClusteringMethods-1994}; \citeauthor{PerkowitzEtzioni-TowardsAdaptiveWebSites-2000}, \cite*{PerkowitzEtzioni-AWSConceptualClusterMining-1999}, \cite*{PerkowitzEtzioni-TowardsAdaptiveWebSites-2000}; \cite{Goethals-SurveyOnFPM-2003,SaglamSalmanSayinTurkay-MixedIntegerClustering-2006,TsayHsuYu-FIUTaNewMethodForFIM-2009}), \clasificacion~\citep{LiuHsu-PostAnalysisOfLearnedRules-1996,LiuHsuMa-IntegratingClassificationAndARM-1998,RokachMaimon-DataMiningWithDecisionTreesTheoryAndApplications2nd-2014} o \regresion~\citep{KohaviQuinlan-DecisionTreeDiscovery-1999}. Los objetivos de la \DM son básicamente \prediccion y descripción, y ambos están estrechamente relacionados. La \prediccion se engloba en las técnicas de \DM supervisadas mientras que la descripción incluye técnicas no supervisadas de \DM como \clasificacion o \indice{visualización}. El nexo entre ambos objetivos es evidente en el análisis de grandes colecciones de datos: si no se realiza un buen proceso descriptivo de los mismos será difícil obtener un buen modelo de \prediccion.

En la \dm se debe plantear bien el problema a resolver. El proceso de \DM es sólo una fase del proceso de \KDD, una pequeña parte del engranaje como muestra la figura~\ref{fig:fasesProcesoKDDFayyad}, pero si no se ejecuta correctamente no puede obtenerse conocimiento válido y previamente desconocido. Una vez seleccionados, preprocesados y transformados los datos y elegida la tarea de \DM que queremos aplicar hemos de decidir con qué algoritmo abordaremos esta fase y qué parámetros se ajustan mejor a los datos existentes y los resultados que esperamos obtener.

Actualmente es una disciplina en la que trabajan investigadores de todas las ramas del saber. Colecciones masivas de datos (en aumento), dispositivos capaces de almacenarlas, distribuirlas y procesarlas y nuevas metodologías para su conversión en conocimiento están al alcance de todos.

La Estadística y la Informática son las ciencias que más aportan a la \dm. No hay estudio de \DM que no intente justificar la bondad de sus resultados con la Estadística, aunque las justificaciones utilizadas no están siempre basadas en un profundo conocimiento de la Estadística. No sería posible la \DM sin el concurso de la Informática, aunque muchas aportaciones no son realmente útiles a la comunidad científica hasta que se desarrollan con eficiencia.

En esta fase se trata de aprovechar la potencia de cálculo de los actuales dispositivos informáticos para analizar relaciones, similitudes y diferencias entre los datos transformados, creando \patrones que pueden ser visualizados de distintos modos para conseguir con ello que el investigador o el propio sistema detecte \patrones específicos o extraños. Perkowitz y Etzioni (\cite*{PerkowitzEtzioni-AWSAutomaticallySynthesizingWebPages-1998}, \cite*{PerkowitzEtzioni-AWSConceptualClusterMining-1999}, \cite*{PerkowitzEtzioni-TowardsAdaptiveWebSites-1999}, \cite*{PerkowitzEtzioni-TowardsAdaptiveWebSites-2000}) utilizan estos \patrones para dividir el sitio web en clusters, conjuntos de páginas que son visitadas de forma conjunta y que son analizadas por el administrador del sitio web para evaluar los resultados y mejorar la \ux si las agrupaciones obtenidas son correctas. Los análisis de \clustering pretenden dividir la población en grupos cuyos individuos sean homogéneos entre sí, de modo que los diferentes clusters sean lo más heterogéneos posible entre sí, basándose en diferentes medidas de similitud. \citet{SchaferKonstanRiedl-ECommerceRecomendation-2001} dividen la gran población de usuarios de un e-comercio en diferentes clusters, de modo que cuando se quiere hacer una recomendación a un usuario se consultará únicamente en el cluster en que mejor se identifique al usuario con lo que el número de datos a procesar será mucho menor y los resultados pueden ser mucho más acertados si se ha hecho correctamente la asignación.

La Estadística es la base de la \dm por lo que muchas de sus técnicas se aprovechan, como el muestreo aplicado a los datos originales en~\citet{OzmutluSpinkOzmutlu-AnalysisOfLargeDataLogs-2002} tras el que aseguran obtener un subconjunto manejable de datos con la misma información que el conjunto original.

La \arm (\ARM) es una técnica de \DM propuesta por~\citet{AgrawalImielinskiSwami-MiningAssociationRulesBetweenSetsOfItemsInLargeDB-1993}. Gran parte de la presente investigación gira en torno a la \ARM. En ciertos trabajos se ha tratado como una tarea análoga a la de \clasificacion~\citep{KohaviQuinlan-DecisionTreeDiscovery-1999}, derivando en la llamada \emph{\Clasificacion Asociativa}~\citep{LiuHsuMa-IntegratingClassificationAndARM-1998}. La \clasificacion se enfoca hacia la \prediccion y la \ARM descubre asociaciones entre los valores de los atributos en estudio, lo que puede aprovecharse en algoritmos como \texttt{CMAR}~\citep{ThabtahCowlingHammoud-ImprovingRuleSorting-2006} o \texttt{AR+OPTBP}~\citep{KahramanliAllahverdi-NewMethodForComposingClassificationRulesARplusOPTBP-2009}.

La ejecución del algoritmo seleccionado no tiene por qué ser única, generalmente se llevarán a cabo diversos ajustes que eviten la obtención de resultados ya conocidos y proporcionen conocimiento previamente desconocido sobre la población en estudio. Cuanto más dinámica sea la población de origen de los datos en proceso más elaborada ha de ser esta fase pues se puede llegar a concluir que el conocimiento previo a este estudio ya no es válido o que es necesario obtener nuevos datos que permitan llegar a nuevas conclusiones del análisis.

Volviendo al proceso de \wum en estudio, una vez tenemos el conjunto de \sns podemos hacer rápidas averiguaciones basándonos en la estadística descriptiva e inferencial para ayudarnos a entender mejor los datos que tenemos. Las \sns son los datos de los que vamos a extraer el conocimiento. Para tener un rápido acceso a ellas lo conveniente es guardarlas en una \db y codificarlas de modo que en lugar de anotar las páginas web con largas cadenas de caracteres se guarde un valor numérico que las identifique unívocamente y que sea fácil de manejar por un dispositivo informático, el uso de códigos \texttt{hash} es muy apropiado para este menester.

% TODO: Añadir alguna cita más para secuencias...
Las \sns representan \secuencias, conjuntos de ítems ordenados que son fácilmente modelables como cadenas de Markov y estudiadas mediante $n$-gramas o analizadas mediante algoritmos adaptados a las \secuencias como \texttt{AprioriAll}, \texttt{AprioriSome} o \texttt{DynamicSome}~\citep{AgrawalSrikant-MiningSequentialPatterns-1995} y \texttt{GSP}~\citep{SrikantAgrawal-MiningSequentialPatternsGeneralizationsAndPerformandeImprovements-1996}. 

El análisis de la \DB de \sns puede estar dirigido a diferentes objetivos, interesándonos en esta fase de la investigación la \prediccion de las páginas web que desean visitar nuestros usuarios. En esta línea se han propuesto muy diversos planteamientos sobre cómo sintetizar la informaión que contiene la (generalmente enorme) \db de sesiones. Entre otras soluciones se propone el uso de \emph{Hypertext Probabilistic Grammar}~\citep{BorgesLevene-DataMiningOfUserNavigationPatterns-1999}, el uso de \emph{Programación Genética Basado en Gramática} (Hervás-Martínez, Romero
Morales y Ventura Soto, \cite*{HervasRomeroVentura-ComparacionMedidasRA-2004}, \cite*{HervasRomeroVentura-SeleccionMedidasEvaluacionReglas-2004}), el sistema de \prediccion \texttt{WhatNext}, basado en $n$-gramas~\citep{SuYangLuZhang-WhatNext-2000} o el modelo \texttt{multi-step dynamic $n$-gram}~\citep{SunChenWenyinMa-IntentionModelingForWebNavigation-2002}.

En nuestros primeros pasos en esta investigación, desarrollados en la sección~\ref{sec:1-3-1-MNW}, nos inclinamos por el uso de \grafos dirigidos para obtener \patrones de navegación de los usuarios en la web. Los \grafos son modelos matemáticos con los que se pueden expresar las conexiones que realizan los usuarios entre las páginas de un sitio web al navegar por él.
%TODO: Sin embargo, al tener en cuenta el orden de visita de cada página nos encontramos con problemas de falta de recursos (básicamente de memoria RAM) que limitaban mucho el número de \sns que se podían tratar simultáneamente.

El uso de \grafos aporta mucha información al proceso de \WUM, sin embargo al seguir creciendo nuestras colecciones de datos se va complicando el uso de este tipo de estructuras. Tras una nueva revisión del estado del arte sobre la materia comprobamos que en muchos estudios se ignoraba este orden, viendo una \sn como un conjunto (no ordenado) de páginas visitadas en una misma sesión. Al no considerar este orden pasamos de tener \secuencias a tener \transacciones, lo que dio lugar a la investigación mostrada en la sección~\ref{sec:1-3-2-RA}.

Al convertir una \sn en una \transaccion se pierde toda la información sobre los enlaces utilizados y sobre el tiempo de permanencia en cada página por lo que hemos de analizar qué información contiene la estructura de datos que hemos seleccionado. Una \transaccion es simplemente un conjunto de datos, en nuestro caso un conjunto de páginas visitadas, ni siquiera se guarda el número de veces que se ha visitado la misma página en una \sn. Ahora cada uno de los registros que tenemos contiene simplemente un conjunto de páginas web que un usuario decidió visitar durante una sesión en nuestro sitio web. Si muchos usuarios agrupan las mismas páginas podremos pensar que hay algo que las relaciona por lo que si un usuario estuviera visitando una de esas páginas podríamos recomendarle visitar el resto de páginas del mismo grupo.

Esto nos obligaba a abordar el uso de técnicas de \arm (\ARM), que permiten extraer información sobre qué páginas están relacionadas entre sí a través del análisis de las \transacciones obtenidas. Variamos nuestra investigación hacia el uso de \ARM por la sencillez del algoritmo \apriori y la fácil comprensión de las reglas obtenidas. Pronto descubrimos que a pesar de la reducción de datos obtenida con este cambio eran todavía muchas las reglas obtenidas y sería difícil tratarlas simultáneamente sin antes aplicar otro tipo de preproceso y transformación de los datos a minar.
