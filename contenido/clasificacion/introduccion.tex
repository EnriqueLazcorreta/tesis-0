% !TEX root = ../../Lazcorreta.Tesis.tex
\ABIERTO
%Los trabajos expuestos en los capítulos anteriores muestran una dificultad presente en muchas investigaciones en el ámbito de la informática, la imposibilidad de comprobar si los resultados obtenidos son correctos y aplicables con la tecnología actual. Todas las Ciencias recogen una Teoría que la sustenta, la Informática también, pero las demostraciones teóricas basadas en otras Ciencias no siempre son válidas para la Informática. Hay muchos artículos teóricos sobre Minería de Datos, pero algunos de ellos no evolucionan en un artículo posterior que muestre cómo se ha realizado el experimento con la tecnología actual usando datos reales.
%
%Es fácil calcular teóricamente el número de reglas de asociación presente en una colección concreta de datos, pero si el algoritmo propuesto no es capaz de almacenar en RAM todas las reglas de asociación del problema no servirá de nada ese algoritmo en esa situación. \texttt{mushroom} fue el primer caso que encontré curioso en el ambicioso campo de la Minería de Datos, una colección de tan solo 5\,644 registros de 23 valores que sólo contenía 100 valores diferentes no podía ser analizada a fondo por el mejor algoritmo de ARM que conozco, Apriori, cuando el número de transacciones distintas que se puede obtener bajo estas circunstancias es X\,XXX\,XXX y con ellas tenemos un máximo de X\,XXX\,XXX\,XXX\,XXX reglas de asociación. La tecnología actual de un equipo de escritorio no puede gestionar en RAM tanta información, pero me negaba a creer que no se pudiera extraer \emph{toda la información} que contuviera un problema tan pequeño. Al profundizar en \texttt{mushroom} y los artículos que lo mencionaban y otras colecciones de datos que encontré publicadas en los mismos portales encontré un modo de indicar al algoritmo de ARM que estoy tratando con un tipo de colecciones de datos especial. No es simplemente una colección de transacciones como las analizadas en~\ref{sec:arm:conceptos-basicos} si no que éstas tienen unas restricciones muy fuertes en su definición.
%
%Este informe refleja el trabajo que he realizado para descubrir lo suficiente como para ser merecedor del título de doctor en Informática. Los anteriores capítulos reflejan una gran labor de documentación y exposición científica de los resultados que podía obtener con colecciones de datos fijas, no disponía de un servidor capaz de poner a prueba nuestras aportaciones y realizar sugerencias en tiempo real a un gran número de usuarios. Podía comprobar que mis cálculos se podían realizar con la tecnología actual y las colecciones de datos que yo manejaba pero no sabía qué ocurriría si aumentaran mis colecciones de datos o si pudiera realizar sugerencias en tiempo real. Es un buen trabajo teórico apoyado en algunas realizaciones prácticas pero del que no puedo extraer aún la conclusión que es un buen trabajo de investigación en Informática.

%TODO: Buscar la cita de gAcademy o eliminar
Uno de los problemas de la búsqueda de \ars mediante equipos informáticos es su necesidad de memoria RAM para guardar todos los \itemsets frecuentes (\aprioriL) en un lugar de rápido acceso para ser más eficientes en la búsqueda de \ars. El \dilemaIR puede aparecer en cualquier momento, dependiendo de los datos que contenga el almacén \D que estemos analizando, y se siguen proponiendo ideas para aliviarlo como el uso de umbrales automáticos para el \soporte propuesto por~\citet{SadhasivamAngamuthu-MiningRareItemsetWithAutomatedSupportThresholds-2011}. Sería interesante poder averiguar, en base a información básica del fichero \D, qué requisitos de memoria RAM necesitamos, de cuáles disponemos y, con ello, deducir hasta qué \soporte mínimo podemos trabajar con esa colección específica de datos en este equipo en particular \citep{JinMcCallenBreitbartFuhryWang-EstimatingTheNumberOfFIInLargeDB-2009,Lhote-NumberOfFPInRandomDBs-2009}. Aunque este proceso puede llevar algo de tiempo y restar eficiencia global al algoritmo de \FIM permite ahorrar el tiempo perdido cuando el programa deja de funcionar por falta de RAM y no es capaz de ofrecernos ningún resultado.

%TODO: Reorganizar, algo de esto se menta en el primer párrafo
%TODO: Ojo, el enlace queda en un salto de página y afecta a la cabecera de la siguiente página. COMPROBARLO EN LA VERSIÓN FINAL
Esta reflexión tiene sentido si pensamos en grandes \datasets pero nos llamó la atención en ficheros tan "`pequeños"' como \texttt{chess.dat} o \mushroom. \mushroom es una colección de datos ampliamente utilizada debido a su publicación en \urlConNotaAlPie{https://archive.ics.uci.edu/ml/datasets/Mushroom}{UCI - Machine Learning Repository}, donde podemos encontrar una completa descripción de los datos en sí y de su aparición en el mundo de la investigación. Encontramos información sobre el uso de este almacén \D en artículos de \clasificacion.

%TODO: Añadir ZakiPetersAssentSeidl_Clicks_06 y Ziani:Ouinten:MiningMaximalFIJavaImplOfFPMAXalgorithm:09 (dea.bib) 
Estos ficheros han sido analizados en muchos artículos desde el punto de vista de la \arm clásica~\citep{Suzuki-DiscoveringInterestingExceptionRulesWithRulePair-2004, Borgelt-EfficientImplementationsOfAprioriAndEclat-2004, ThabtahCowlingHammoud-ImprovingRuleSorting-2006, WangXinCoenen-MiningEfficientlySignificantCAR-2008, LiZhang-MiningMaximalFIOnGraphicsProcessors-2010, MalikRaheja-ImprovingPerformanceOfFrequentItemsetAlgorithm-2013, RituArora-IntensificationOfExecutionOfFrequentItemSetAlgorithms-2014, SahooKumarGoswami-AnAlgorithmForMiningHighUtilityClosedItemsetsAndGenerators-2014}. Se proponen nuevas medidas, como la \emph{utilidad} de los \itemsets~\citep{WuShie-MiningTopKHighUtilityItemsets-2012} para aliviar el \dilemaIR. En todos los casos se ha de recurrir al \soporte mínimo para poder llevar a cabo el análisis en un tiempo prudencial (algo más de 2 segundos en el caso de~\citeauthor{RituArora-IntensificationOfExecutionOfFrequentItemSetAlgorithms-2014}, un estudio muy reciente cuyos gráficos coinciden con los de~\citeauthor{MalikRaheja-ImprovingPerformanceOfFrequentItemsetAlgorithm-2013}). Intentamos aplicar técnicas de \dm a colecciones relativamente pequeñas (\mushroom sólo contiene $8\,124 \times 23$ datos combinando 119 ítems distintos) y no podemos profundizar o trabajar en tiempo real si no aplicamos recortes drásticos de información. Hoy en día, almacenes \D de este tamaño deberían poder analizarse en tiempos razonables sin prescindir de ninguno de sus datos.

Antes de obtener más información sobre \mushroom comenzamos a analizar los resultados obtenidos en nuestros propios experimentos sobre este almacén \D. Al observarlos encontramos una estructura concreta: todas las filas ("`\transacciones"') tienen el mismo número de datos y en la primera columna sólo aparece un 1 o un 2, en la segunda sólo un 3, 4 o 5\ldots Se trata de una estructura muy rígida y con exceso de información. Al descubrir que el 1 aparecía en 3\,916 \transacciones dedujimos y comprobamos que el 2 aparecería en las restantes 8\,124 - 3\,916 . En el proceso de \fim estábamos desperdiciando esta información y nos entreteníamos en contar el número de veces que aparecía el ítem 2. También descubrimos que el ítem 3 aparecía en un total de 3\,656 \transacciones, en 1\,708 ocasiones junto al ítem 1, luego en las restantes 3\,656 - 1\,708 veces que aparece ha de estar junto al ítem 2. No necesitamos averiguar nada sobre el ítem 2, lo que averigüemos sobre el ítem 1 y la información estructural del \dataset nos dará toda la información que tiene \D sobre el ítem 2. No necesitamos utilizar memoria RAM para guardar información sobre el ítem 2, tendremos memoria RAM libre para analizar los \irs más frecuentes de \D o incluso para obtener información sobre todos los ítems en estudio.

Nuestros primeros análisis sobre \mushroom cumplieron con nuestras expectativas, tras la primera lectura de \D podíamos comprobar que las \transacciones tenían una estructura que podía ser utilizada para ahorrar recursos de memoria y obtuvimos \emph{todas} las \ARs que tiene el fichero sin tener que recurrir al uso de \soporte mínimo, todo ello en pocos segundos y con un consumo de RAM aceptable para cualquier equipo de sobremesa.

%TODO: Revisar esta información
\borrar{Mejorar}La Minería de Reglas de \Clasificacion (CRM) toma pronto las bondades de la Minería de Reglas de Asociación (ARM) derivando en una nueva línea de investigación en auge denominada \carm (\CARM) ~\citep{LiuHsuMa-IntegratingClassificationAndARM-1998, Bayardo-EfficientlyMiningLongPatternsFromDB-1998, WangXinCoenen-MiningEfficientlySignificantCAR-2008} donde se aprovecha el potencial que tienen las \ars para descubrir clasificadores precisos. CARM transforma los \datasets que han de ser analizados desde la perspectiva de CRM para que puedan ser vistos como conjuntos de transacciones y hacer un análisis basado en ARM. Para ello convierten los valores utilizados en CRM, formado por pares atributo=valor, en atributos binarios que tendrán el significado "`Si aparece el atributo binario X entonces el registro toma el valor Y en el atributo original $\A_i$"'. Ha habido grandes avances en \Clasificacion gracias a esta pequeña transformación, sin embargo también se ha perdido información muy importante sobre el \dataset en estudio, que no será aprovechada si no la incorporamos a la colección de transacciones que queremos analizar con ARM.

Tras llevar a cabo la investigación que se expone en este capítulo podemos afirmar que, aunque muchas de las citas revisadas han experimentado con \mushroom para aliviar su \dilemaIR, \mushroom y muchos \datasets de similares características realmente no contienen \IRs y contienen información redundante que puede ser ignorada para trabajar con más profundidad sobre el problema de \Clasificacion para el que han sido creados. 

Por último, introducimos y modelizamos los conceptos de \Catalogo y \CC con los que se puede analizar de forma más eficiente la información contenida estos ficheros y mejorar el proceso de \Clasificacion en muchos de los problemas ya estudiados y en todos aquellos que se puedan plantear en el futuro siguiendo las pautas marcadas en esta investgación. 
